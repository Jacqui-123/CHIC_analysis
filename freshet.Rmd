## Reference Hydrometric Basin Network - Freshet calculations

Development of a global freshet calculation method


Jacqui Levy

```{r, echo=FALSE, include = FALSE, warning= FALSE}

library(IHA)
library(tidyverse)
library(tidyhydat)
library(zoo)
library(lubridate)
library(ggplot2)
library(dataRetrieval)
library(Kendall)
library(knitr)
library(plotly)
library(data.table)
library(manipulateWidget)
library(cluster)
library(factoextra)
library(dendextend) #for comparing two dendrograms

#install.packages("IHA", repos="http://R-Forge.R-project.org")

```

```{r}


#freshet ideas:
#calculate freshet inflection - try shorter running means, first day when flow is 2x the previous day, see how long that happens for and do a midpoint
#clustering using regime shape?
#try all freshet using regime shape

#https://builtin.com/machine-learning/agglomerative-clustering
#agglomerative hierarchical - bottom-up, each individual data point is its own cluster. Merged based on similarities between the distance or proximity matrix until their is one cluster. Really good at finding out small groups. but is computationally expensive because it clusters and says when a cluster is finished. Results are reproducible, vs k means which they are not. 

#hierarchical is good at detecting anomalies - can root out errors/outliers if a cluster doesn't belong to any cluster. 
#used in ecology, or in marketing - to group customers based on shared traits to inform marketing campaigns. 

#(vs divisive which considers all clusters one cluster then splits)
#(k means is good for larger datasets bc low computation complexity so it's faster)


```


# Part 1-Regime Shape (Median)

```{r}

#use df from rhbn.Rmd -> added doy, wy, df_tidied function.  

#add a column of weeks of the year to later group by 
stn_all <- df_ready %>% 
  group_by(STATION_NUMBER, waterYear) %>%
  dplyr::mutate(weeks = rep(1:(ceiling(n()/7)), each = 7)[1:n()]) %>%  #ceiling always rounds up
#make a new column, and mutate from 1 to the result of ceiling(n()/7), ie 1:52
  mutate(weeks = if_else(weeks == 53, 52, weeks)) %>% #make the one day week 53 be week 52. Issue is that then there are 9 days in week 52 in leap years. Could just omit feb 29th completely?
  ungroup()

```

Get RHBN stations 
```{r}
#from govt canada RHBN list: https://www.canada.ca/en/environment-climate-change/services/water-overview/quantity/monitoring/survey/data-products-services/reference-hydrometric-basin-network.html

stns <- read.csv("data/RHBN_Metadata.csv" ) %>% filter(DATA_TYPE == "Q") 

stns <- stns %>%
  mutate(STATION_NUMBER = toupper(STATION_NUMBER)) %>%
  distinct(STATION_NUMBER, .keep_all = TRUE) %>%
  arrange()

```

Get unique stn numbers and put into a list to get gross discharge

```{r}
#put station numbers into a list, by ecoregion

stn_num_s <- stns %>% 
  #filter(Data.Quality != "S") %>%
  mutate(Ecoregion = ifelse(Ecoregion == '', "No Ecoregion", Ecoregion)) %>%
  distinct(STATION_NUMBER, .keep_all = TRUE) %>%
  group_by(Ecoregion) %>%
  summarise(STATION_NUMBER = list(STATION_NUMBER))

#loop through this and get several vectors of station ids

ecoreglist <- list()
for (i in unique(stn_num_s$Ecoregion)) {
    
    df_subset <- stn_num_s[stn_num_s$Ecoregion == i,]
    vec <-  as.vector(unlist(df_subset$STATION_NUMBER))
    names(vec) <- i
    ecoreglist[[i]] <- vec
}


```


```{r}


#Use tidyhydat database to get daily flows for all ecoregions and put all dataframes in a list
eastern_temp_forests <- tidyhydat::hy_stations(station_number = ecoreglist$`Eastern Temperate Forests`)
great_plains <- tidyhydat::hy_stations(station_number = ecoreglist$`Great Plains`)
hudson_plains <- tidyhydat::hy_stations(station_number = ecoreglist$`Hudson Plains`)
w_coast_forests <- tidyhydat::hy_stations(station_number = ecoreglist$`Marine West Coast Forests`)
no_ecoregion <- tidyhydat::hy_stations(station_number = ecoreglist$`No Ecoregion`)
n_american_deserts <- tidyhydat::hy_stations(station_number = ecoreglist$`North American Deserts`)
northern_forests <- tidyhydat::hy_stations(station_number = ecoreglist$`Northern Forests`)
nw_forested_mtns <- tidyhydat::hy_stations(station_number = ecoreglist$`Northwest Forested Mountains`)
taiga <- tidyhydat::hy_stations(station_number = ecoreglist$Taiga)
tundra <- tidyhydat::hy_stations(station_number = ecoreglist$Tundra)

stn_all_drainage <- bind_rows(eastern_temp_forests, great_plains, hudson_plains, w_coast_forests, no_ecoregion, n_american_deserts, northern_forests, nw_forested_mtns, taiga, tundra )

rm(ecoreglist, stn_num_s, df_subset, stns)

rm(eastern_temp_forests, great_plains, hudson_plains, w_coast_forests, no_ecoregion, n_american_deserts, northern_forests, nw_forested_mtns, taiga, tundra )

#join
stn_flow_disch <- left_join(stn_all, stn_all_drainage, by = "STATION_NUMBER")  %>% select(-c(ends_with(".x")))


```

```{r}

stn_flow_disch_c <- stn_flow_disch %>%
  filter(Data.Quality == "C")

rm(stn_flow_disch)

```


```{r}


#check nas
check_nas <- function(df){
  df <- {{df}}
  df <- as.data.frame(df)
  nas <- df[is.na(df$DRAINAGE_AREA_GROSS),]
  col <- as.data.frame(colSums(is.na(nas))) %>% rename("sum_nas" ="colSums(is.na(nas))") %>%
    filter(sum_nas == 0)
    stn_num <- unique(nas$STATION_NUMBER)
    
  if (length(stn_num) > 0) {
    warning("There are Nas for DRAINAGE_AREA_GROSS in the following station numbers: ", paste0(stn_num, collapse = ", ") )
  } else {print("todo bien, no hay na's en estes estaciones")}
    rm(stn_num, df, nas, col)
}


```


```{r}
#Use the functions built for this project to delete years with more than 14 consecutive days of missing data, and the tidyverse package to estimate missing observations using the last existing observation.

stn_cln <- stn_flow_disch_c

#delete years that have >14 days missing data using calc_rle function from functions I built for this project
lst_stns_cln <- split(stn_cln, stn_cln$STATION_NUMBER )
lst_stns_cln_rle <- lapply(lst_stns_cln, calc_rle)

#unlist the station numbers
stns_ready <- bind_rows(lst_stns_cln_rle, .id = "STATION_NUMBER") 

rm(stn_cln, lst_stns_cln,lst_stns_cln_rle )

#remove station numbers with nas for gross discharge values
#check_nas(stns_ready)

```


```{r}

#reformat date and fill remaining missing values with preceding values
stns_daymonthyear_full <- stns_ready %>%
  filter(STATION_NUMBER != "08OA003" & STATION_NUMBER != "10ED007") %>%
  mutate(Date = format(Date,"%d-%m-%Y")) %>%
  group_by(STATION_NUMBER) %>%
  fill(Value, .direction = 'downup') %>%
  #calculate total discharge(runoff) 
  mutate(runoff = ((Value * 3600 * 24) / (DRAINAGE_AREA_GROSS * 1000000) )) 

#to see how many years in each station - get rid of stations with not enough data
#stns_daymonthyear_full %>%
 # group_by(STATION_NUMBER) %>%
#  summarise(Year = toString(unique(waterYear)))

check_nas(stns_ready)


```

```{r}

#weekly medians for each station's runoff, then for each week, calculate the z-score across all stations

#calculate the weekly medians for each station and year, and then the station-wide medians
stns_full_meds <- stns_daymonthyear_full %>%
  group_by(STATION_NUMBER, waterYear, weeks) %>%
  dplyr::summarise(wkly_median = median(runoff))%>%
 group_by(STATION_NUMBER, weeks) %>% 
 dplyr::summarise(meta_med = median(wkly_median))


n_distinct(stns_full_meds$STATION_NUMBER)
#738


#translate to z-score - use this df to graph later on
stns_full_zf <- stns_full_meds %>%
  group_by(STATION_NUMBER) %>%
  #filter(STATION_NUMBER == "01AD002") %>%
  mutate(z_scores = scale(meta_med)) %>%
select(STATION_NUMBER, weeks, z_scores) #%>%
 # na.omit()


#check by calculating the mean and SD for each week across all sites
sd_mn <- stns_full_zf %>%
  group_by(STATION_NUMBER) %>%
  mutate( sd = round(sd(z_scores),8)) %>%
  mutate(mn = round(mean(z_scores),8))

if(!all(sd_mn$mn == 0)) {print("warning, mean values are not all equal to 0")} else {print("mean values equal 0")}
if(!all(sd_mn$sd  == 1)) {print("warning, sd values are not all equal to 1")} else {print("std values equal 1")}

check_nas(stns_full_zf)

```



```{r}

#pivot so the weeks are columns, and stn numbers are the rows, and z-score by weeks
stns_full <- stns_full_zf %>%
pivot_wider(names_from = weeks, values_from = z_scores) %>%
  column_to_rownames('STATION_NUMBER')
#use na.omit if needed


#stns_full <- as.data.frame(scale(stns_full)) 

#rowMeans(stns_full[1,])
#round(apply(stns_full, 1, sd), 8)


```


```{r}

#check by calculating the mean and SD for each week across all sites
#mean should be 0, sd should be 1
#mn <- round(colMeans(stns_full),8)
#sd <- round(apply(stns_full, 2, sd), 8)

#if(!all(mn == 0)) {print("warning, mean values are not all equal to 0")} else {print("mean values equal 0")}
#if(!all(sd == 1)) {print("warning, sd values are not all equal to 1")} else {print("std values equal 1")}

```


####clustering - medians
https://uc-r.github.io/hc_clustering 

```{r}
#find strongest linkage method ie way of measuring dissimilarity 
#code taken from https://uc-r.github.io/hc_clustering

m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

ac <- function(x) {
  agnes(stns_full, method = x)$ac 
}

map_dbl(m, ac)

#ward method has highest ac


```


```{r}

#clustering for part 1 - hierarchical agglomerative

#dist <- dist(stns_full_z, method = "euclidean")
#hclust <- hclust(dist, method = "complete")
#plot(hclust, cex = 0.6, hang = -1)
#clustering on the distance matrix gives same results as clustering on the z-scored data

hclust <- agnes(stns_full, method = "ward") 


hclust$ac #likely 4-6 clusters 

factoextra::fviz_dend(hclust, cex = 0.4, k = 4, k_colors = viridis::viridis_pal(option = 'D')(4),
main = "Dendogram for agglomerative hierarchical clustering \n (distance matrix using ward linkage method), agg coeff = .99")


```

```{r}

library(factoextra)
ka <- 3
#plot cluster with groups = ka
sub_grp <- cutree(hclust, k = ka)

clusters_3 <- cutree(hclust, k=ka)

#cutree(as.hclust(hclust), k = 3)

fviz_nbclust(stns_full, FUN = hcut, method = "wss", k.max = 10) #minimizes within cluster sum squares 

#silhouette score, ranges from -1 to 1 and says how far apart from one another the clusters are. 1/-1. >.7 means they are easily distinguished from one another
fviz_nbclust(stns_full, FUN = hcut, method = "silhouette", k.max = 10)

factoextra::fviz_dend(hclust, cex = 0.4, k = ka, k_colors = viridis::viridis_pal()(ka),
main = "Dendogram for agglomerative hierarchical clustering \n (ward linkage method), ac = 0.99")

gapstat <- fpc::cluster.stats(stns_full, sub_grp)
gapstat$avg.silwidth #.12 how similar a cluster is to its own cluster, goes from -1 to +1
gapstat$ch   #-124 Gap stat tries to minimize the number of clusters

#for (k in 2:10) {
 # clusters <- cutree(hclust, k = k)
#  stats <- fpc::cluster.stats(stns_full, clusters)
#  cat("Clusters:", k, "avg sil width", stats$avg.silwidth, "connectivity index:", stats$ch, "\n")
#}

```

####PCA - medians
```{r}

pca_result <- prcomp(stns_full, scale. = TRUE)

summary(pca_result)
pca_result$rotation

data <- data.frame(pc1 = pca_result$x[,1], pc2 = pca_result$x[,2], Cluster = as.factor(clusters_3))

plot(data$pc1, data$pc2, col = data$Cluster, pch = 19)


ggbiplot::ggscreeplot(pca_result, type = "pev") + 
  theme_classic()+
  scale_x_continuous(breaks=c(1,2,3,4,5,6,7,8,9,10)) +
  xlab("Principal component number") +
  ylab("Proportion of explained variance")


ggplot(data, aes(x = pc1, y = pc2, colour = Cluster)) +
  geom_point() +
  viridis::scale_color_viridis(discrete = TRUE) +
  theme_bw() 

```
####Graph clustering results
Line graph
```{r}

#stns_full_pivot <- stns_full %>%
#  rownames_to_column("STATION_NUMBER") %>%
#pivot_longer(cols = -STATION_NUMBER, names_to = 'weeks', values_to = 'z_scores') 


full_z_clusters <- as.data.frame(sub_grp) %>% 
  dplyr::rename(cluster_number = sub_grp) %>%
  rownames_to_column('STATION_NUMBER') %>%
  left_join(stns_full_zf, by = 'STATION_NUMBER') %>%
  group_by(cluster_number, weeks) %>%
  dplyr::summarize(cluster_median = median(z_scores)) %>% 
  complete(weeks = seq(1:52))


#full_z_clusters <- as.data.frame(sub_grp) %>% 
 # dplyr::rename(cluster_number = sub_grp) %>%
#  rownames_to_column('STATION_NUMBER') %>%
 # left_join(stns_full_pivot, by = 'STATION_NUMBER') %>%
#  group_by(cluster_number, weeks) %>%
#  dplyr::summarize(cluster_median = median(z_scores)) 



#graph weekly z scores by cluster only 
#y = cluster_median
ggplot(full_z_clusters, aes(y = cluster_median, x = as.numeric(weeks), color = as.factor(cluster_number ))) +
  geom_line(size = 1) +
  theme_bw() +
 ylab('Cluster median runoff (z-scores)') + 
   xlab('Week number') +
  ggtitle(" ") +
      theme(plot.title = element_text(hjust = 0.5, size = 12)) +
  scale_colour_viridis_d() +
  labs(color = "Cluster Number") +
  ggtitle("Regime Shape - Weekly Medians \n by Cluster (z-scores)")

#combined <- m + z + plot_layout(guides = "collect") & theme(legend.position = "bottom")

```

Boxplots
```{r}


ggplot(full_z_clusters, aes(x = weeks, y= cluster_median, fill = as.factor(cluster_number))) +
  geom_boxplot() +
  stat_boxplot(geom = "errorbar") +
  facet_wrap(~cluster_number) + 
    theme_bw() +
 #ylab('Z-score of IHA variables') + 
   xlab('') +
  ggtitle(" ") + 
      theme(plot.title = element_text(hjust = 0.5, size = 12)) +
  labs(color = "Cluster Number") +
  #ggtitle("IHA variables") +
  theme(axis.text.x = element_text(angle= 90)) +
 viridis::scale_fill_viridis(discrete = TRUE) +
  theme(legend.position = "none")


```
####Lat-Longs 
```{r}

stn_flow_disch_c <- stn_flow_disch_c %>%
  select(-c(Date, Value, CONTRIBUTOR_ID, OPERATOR_ID, DATUM_ID, DRAINAGE_AREA_EFFECT, HYD_STATUS, SED_STATUS, Parameter, REAL_TIME, day_of_year, Symbol, weeks,Years.S , waterYear, Symbol)) %>%
  distinct()

latlongcluster_3 <- as.data.frame(sub_grp) %>% 
  dplyr::rename(cluster_number = sub_grp) %>%
  rownames_to_column('STATION_NUMBER') %>%
  left_join(stns_full_zf, by = 'STATION_NUMBER') %>%
  select(-c(weeks, z_scores)) %>%
  distinct() %>%
  left_join(stn_flow_disch_c, by = "STATION_NUMBER")

colSums(is.na(latlongcluster_3))

latlongcluster[is.na(latlongcluster$cluster_number),]

#write.csv(latlongcluster, "data/clusters_lat_long_cont_stns.csv")

```

All stns lat-longs

Get RHBN stations 
```{r}

#from govt canada RHBN list: https://www.canada.ca/en/environment-climate-change/services/water-overview/quantity/monitoring/survey/data-products-services/reference-hydrometric-basin-network.html

stns <- read.csv("data/RHBN_Metadata.csv" ) #%>% filter(DATA_TYPE == "Q") 

stns <- stns %>%
  mutate(STATION_NUMBER = toupper(STATION_NUMBER)) %>%
  distinct(STATION_NUMBER, .keep_all = TRUE) %>%
  arrange()

```

Get unique stn numbers and put into a list to get gross discharge

```{r}
#put station numbers into a list, by ecoregion

stn_num_s <- stns %>% 
  #filter(Data.Quality != "S") %>%
  mutate(Ecoregion = ifelse(Ecoregion == '', "No Ecoregion", Ecoregion)) %>%
  distinct(STATION_NUMBER, .keep_all = TRUE) %>%
  group_by(Ecoregion) %>%
  summarise(STATION_NUMBER = list(STATION_NUMBER))

#loop through this and get several vectors of station ids

ecoreglist <- list()
for (i in unique(stn_num_s$Ecoregion)) {
    
    df_subset <- stn_num_s[stn_num_s$Ecoregion == i,]
    vec <-  as.vector(unlist(df_subset$STATION_NUMBER))
    names(vec) <- i
    ecoreglist[[i]] <- vec
}

```


```{r}

#Use tidyhydat database to get daily flows for all ecoregions and put all dataframes in a list
eastern_temp_forests <- tidyhydat::hy_stations(station_number = ecoreglist$`Eastern Temperate Forests`)
great_plains <- tidyhydat::hy_stations(station_number = ecoreglist$`Great Plains`)
hudson_plains <- tidyhydat::hy_stations(station_number = ecoreglist$`Hudson Plains`)
w_coast_forests <- tidyhydat::hy_stations(station_number = ecoreglist$`Marine West Coast Forests`)
no_ecoregion <- tidyhydat::hy_stations(station_number = ecoreglist$`No Ecoregion`)
n_american_deserts <- tidyhydat::hy_stations(station_number = ecoreglist$`North American Deserts`)
northern_forests <- tidyhydat::hy_stations(station_number = ecoreglist$`Northern Forests`)
nw_forested_mtns <- tidyhydat::hy_stations(station_number = ecoreglist$`Northwest Forested Mountains`)
taiga <- tidyhydat::hy_stations(station_number = ecoreglist$Taiga)
tundra <- tidyhydat::hy_stations(station_number = ecoreglist$Tundra)

stn_all_drainage <- bind_rows(eastern_temp_forests, great_plains, hudson_plains, w_coast_forests, no_ecoregion, n_american_deserts, northern_forests, nw_forested_mtns, taiga, tundra )

rm(ecoreglist, stn_num_s, df_subset, stns)

rm(eastern_temp_forests, great_plains, hudson_plains, w_coast_forests, no_ecoregion, n_american_deserts, northern_forests, nw_forested_mtns, taiga, tundra )

#join
stn_flow_disch <- left_join(stn_all, stn_all_drainage, by = "STATION_NUMBER")  %>% select(-c(ends_with(".x")))

```

Merge dfs 
```{r}

latlongcluster_6 <- latlongcluster_6 %>%
  rename("cluster_number_6" = "cluster_number")
latlongcluster_3 <- latlongcluster_3 %>%
  rename("cluster_number_3" = "cluster_number")

clusters <- merge(latlongcluster_6, latlongcluster_3, on = "STATION_NUMBER")

stn_flow_disch <- stn_flow_disch %>%
  select(-c(Date, Value, CONTRIBUTOR_ID, OPERATOR_ID, DATUM_ID, DRAINAGE_AREA_EFFECT, HYD_STATUS, SED_STATUS, Parameter, REAL_TIME, day_of_year, Symbol, weeks, waterYear, Symbol))  %>%
  distinct()

stn_flow_disch <- stn_flow_disch %>%
  group_by(STATION_NUMBER) %>%
  summarise(across(everything(), ~na.omit(.)[1]), .groups = "drop")

allclust <- merge(stn_flow_disch, clusters, on = 'STATION_NUMBER', all.x = TRUE) 

allclust <- allclust %>%
  group_by(STATION_NUMBER) %>%
  summarise(across(everything(), ~na.omit(.)[1]), .groups = "drop")

#write.csv(allclust, "data/allclust.csv")

```


#workflow for multiple clusters
```{r}

ka <- 6
#plot cluster with groups = ka
sub_grp <- cutree(hclust, k = ka)

#clusters_3 <- cutree(hclust, k=ka)

latlongcluster_6 <- as.data.frame(sub_grp) %>% 
  dplyr::rename(cluster_number = sub_grp) %>%
  rownames_to_column('STATION_NUMBER') %>%
  left_join(stns_full_zf, by = 'STATION_NUMBER') %>%
  select(-c(weeks, z_scores)) %>%
  distinct() %>%
  left_join(stn_flow_disch_c, by = "STATION_NUMBER") %>%
    rename("cluster_number_6" = "cluster_number")

clusters_3_4 <- merge(latlongcluster_3, latlongcluster_4, on = "STATION_NUMBER")
clusters_5_6 <- merge(latlongcluster_5, latlongcluster_6, on = "STATION_NUMBER")
clusters_3_to_6 <- merge(clusters_3_4, clusters_5_6, on = "STATION_NUMBER")
rm( clusters_3_4,clusters_5_6, latlongcluster_3, latlongcluster_4 )

stn_flow_disch <- stn_flow_disch %>%
  select(-c(Date, Value, CONTRIBUTOR_ID, OPERATOR_ID, DATUM_ID, DRAINAGE_AREA_EFFECT, HYD_STATUS, SED_STATUS, Parameter, REAL_TIME, day_of_year, Symbol, weeks, waterYear, Symbol))  %>%
  distinct()

stn_flow_disch <- stn_flow_disch %>%
  group_by(STATION_NUMBER) %>%
  summarise(across(everything(), ~na.omit(.)[1]), .groups = "drop")

allclust <- merge(stn_flow_disch, clusters_3_to_6, on = 'STATION_NUMBER', all.x = TRUE) 

allclust <- allclust %>%
  group_by(STATION_NUMBER) %>%
  summarise(across(everything(), ~na.omit(.)[1]), .groups = "drop") 


#clusters_no_nas <- allclust[!is.na(allclust$cluster_number_3),]

#write.csv(allclust, "allclust.csv")

```


```{r}

```


####map clustering- full yr data

```{r}

#add subgroup (cluster) column to the df, join with lat-long and map
stns_full_clusters <- stns_full_z %>%
 dplyr::mutate(cluster_number = sub_grp) %>%
  rownames_to_column('STATION_NUMBER')

full_clust_latlong <- left_join(stns_full_clusters, stn_drainage, by = "STATION_NUMBER")

map_function(full_clust_latlong)


```


##Fort McMurray
```{r}
stn_07DA001 <- tidyhydat::hy_daily_flows(station_number = "07DA001") %>% 
  mutate(runoff = (Value/(133000*1000*1000)*1000*86400) ) #133,000 = Gross Drainage area in tidyhydat


#stn_07DA001 <- add_water_year(stn_07DA001, 10) 

stn_07DA001 <- stn_07DA001 %>%
  mutate(Year = year(Date)) %>%
  rename(waterYear = Year)

stn_07DA001 <- calc_day_of_wyear(stn_07DA001)
#Warning in calc_day_of_wyear(stn_07DA001) :
#STATION_NUMBER: 07DA001, waterYear: 1957, Values: 92
#STATION_NUMBER: 07DA001, waterYear: 1959, Values: 335
#STATION_NUMBER: 07DA001, waterYear: 1963, Values: 334
#STATION_NUMBER: 07DA001, waterYear: 1964, Values: 306

stn_07DA001 <- stn_07DA001 %>%
  select(-c(Value)) %>%
  rename(Value = runoff)

Group_3_freshet(stn_07DA001)
```

#log transform data
```{r}

#check for 0s and Nas
check_nas <- function(df, col_name, years_column){

  indices <- which(is.na(df[[col_name]]))

  if (length(indices) > 0){
  years <- unique(df[[years_column]][indices])
  
  warning("There are NAs for the input column for the col_name for the following years:", "\n", paste(years, collapse = ","))
}else {cat("No Na values found in",col_name, "\n")}
}

check_nas(stn_07DA001, col_name = 'Value', years_column = 'waterYear')

```


```{r}

check_zeroes <- function(df, col_name, years_column){
  indices <- which(df[[col_name]]==0)
  
  if (length(indices) > 0){
  years <- unique(df[[years_column]][indices])
  
  warning("There are 0s for the input column for the value column for the following waterYears:\n", paste(years, collapse = ","))
}else {cat("No Na values found in", col_name, "column\n")}
}

check_zeroes(stn_07DA001, col_name = 'Value', years_column = 'waterYear')


```

```{r}

#log transform
stn_07DA001_log <- stn_07DA001 %>%
  filter(waterYear != 1969 & waterYear != 1969) %>%
  mutate(log_runoff = log(Value)) 

Group_3_freshet(stn_07DA001_log)

```

```{r}

tidyhydat::hy_stations(station_number = "07DA001")
tidyhydat::hy_stn_data_coll(station_number = "07DA001")
tidyhydat::hy_stn_regulation(station_number = "07DA001")

```


```{r}
stn_07DA001_1974 <- stn_07DA001 %>%
  filter(waterYear == 1974) %>%
  filter(month(Date) %in% c(3,4,5,6)) %>%
  select(Value, day_of_year)

model <- lm(data = stn_07DA001_1974, Value ~ day_of_year )

predicted <- predict(model)

plot(stn_07DA001_1974$day_of_year)
lines(stn_07DA001_1974$day_of_year, predicted)

dy <- diff(stn_07DA001_1974$Value) /diff(stn_07DA001_1974$day_of_year)

inflect_index <- which(diff(sign(dy)) != 0) +1

points(stn_07DA001_1974$day_of_year[inflect_index], stn_07DA001_1974$Value[inflect_index], pch = 16, col ="blue")
legend("topleft", legend = "Inflection Point", pch = 16)

726.68/3
```


```{r}

data.frame(day_of_year = stn_07DA001_1974$day_of_year[inflect_index],
           Value = stn_07DA001_1974$Value[inflect_index])


```

```{r}


smoothed <- loess(Value ~ day_of_year, data = stn_07DA001_1974, span = 0.5)
predicted_sm <- predict(smoothed)
dy_dx <- diff(predicted_sm)/diff(stn_07DA001_1974$day_of_year)
inflect_sm <- which(diff(sign(dy_dx)) != 0) + 1

data.frame(day_of_year = stn_07DA001_1974$day_of_year[inflect_sm],
           Value = stn_07DA001_1974$Value[inflect_sm])


```


```{r}
smoothed <- loess(Value ~ day_of_year, data = stn_07DA001_1974, span = 0.5)
predicted_sm <- predict(smoothed)
dy_dx <- diff(predicted_sm)/diff(stn_07DA001_1974$day_of_year)
dx_dy2 <- diff(dy_dx) / diff(stn_07DA001_1974$day_of_year[1:length(stn_07DA001_1974$day_of_year)-1])
infle_in <- which(diff(sign(dx_dy2)) != 0) +1

data.frame(day_of_year = stn_07DA001_1974$day_of_year[infle_in],
           Value = stn_07DA001_1974$Value[infle_in])



```

```{r}
```


```{r}
```

